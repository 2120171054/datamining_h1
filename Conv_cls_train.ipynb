{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a363a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import os,cv2\n",
    "import torch.nn as nn\n",
    "from cyvlfeat.kmeans import kmeans,kmeans_quantize\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "uncertianty=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca36fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats=np.load('record_feats_train.npy').astype(np.float32)\n",
    "data=np.reshape(feats,(feats.shape[0],-1))\n",
    "\n",
    "if  not os.path.exists('sparse_labels.npy'):\n",
    "    print('clustering')\n",
    "    K=10\n",
    "    centers=kmeans(data,num_centers=K,initialization='PLUSPLUS',num_repetitions=10,\n",
    "                   max_num_comparisons=100,max_num_iterations=100,algorithm='LLOYD',num_trees=3)\n",
    "    labels=kmeans_quantize(data,centers)\n",
    "    # to get the sparse matrix of labels\n",
    "    sparse_labels=np.eye(K)[labels]\n",
    "    labels=labels.astype(np.float32)\n",
    "    sparse_labels=sparse_labels.astype(np.float32)\n",
    "\n",
    "    np.save('sparse_labels.npy',sparse_labels)\n",
    "    np.save('labels.npy',labels)\n",
    "else:\n",
    "    sparse_labels=np.load('sparse_labels_.npy').astype(np.float32)\n",
    "    labels=np.load('labels_.npy').astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f563ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "train_data_loadpath='/mnt/sda/sunche/datasets/Anomaly/Avenue/train/'\n",
    "train_label_loadpath='/mnt/sda/sunche/datasets/Anomaly/Avenue/train/'\n",
    "root='/mnt/sda/sunche/datasets/Anomaly/Avenue/'\n",
    "def Gaussian(X,rate=0.05):\n",
    "    # Salt and pepper noise\n",
    "    drop = np.random.normal(0,rate, X.shape)\n",
    "    X=X+drop\n",
    "    return X\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, train_data_loadpath,labels,transform=None, target_transform=None,):\n",
    "        self.path=train_data_loadpath\n",
    "        self.files=os.listdir(train_data_loadpath)\n",
    "        self.inds=range(len(self.files))\n",
    "        self.labels=labels\n",
    "    def __getitem__(self, index):\n",
    "        img_path=os.path.join(self.path,str(self.inds[index]+1)+'.jpg')\n",
    "        img = cv2.resize(cv2.imread(img_path),(224,224))\n",
    "        img_npy=np.array(img).astype(np.float32)/255.0\n",
    "        \n",
    "#         img_npy = Gaussian(img_npy)\n",
    "        img_npy = np.reshape(img_npy,[224,224,3])\n",
    "        img_npy = np.transpose(img_npy,[2,0,1])\n",
    "        label=self.labels[index]\n",
    "\n",
    "        img_npy=torch.from_numpy(img_npy).float()\n",
    "#         label=torch.from_numpy(label).float()\n",
    "        return img_npy,label\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "# torch.set_num_threads(1)\n",
    "# train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE,num_workers=16, shuffle=True,pin_memory=False)   \n",
    "\n",
    "\n",
    "# class MyDataset(Dataset):\n",
    "#     def __init__(self, feats, labels):\n",
    "#         self.feats=feats\n",
    "#         self.labels=labels\n",
    "#     def __getitem__(self, index):\n",
    "#         return self.feats[index],self.labels[index]\n",
    "#     def __len__(self):\n",
    "#         return len(self.feats)\n",
    "BATCH_SIZE=64\n",
    "if uncertianty==False:\n",
    "    train_data=MyDataset(train_data_loadpath,labels)\n",
    "if uncertianty==True:\n",
    "    train_data=MyDataset(train_data_loadpath,sparse_labels)\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE,num_workers=16, shuffle=True,pin_memory=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37449a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = data.shape[1], 10, 256\n",
    "import auto_encoders\n",
    "model = auto_encoders.VGG_11(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4368f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if uncertianty==False:\n",
    "    epoch_=100\n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "    model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-3,weight_decay=0)\n",
    "    loss_func = nn.CrossEntropyLoss() \n",
    "    loss_func.cuda() \n",
    "    num=0\n",
    "    model.train()\n",
    "    record_loss=[]\n",
    "    for epoch in range(epoch_):\n",
    "        print('epoch {}'.format(epoch + 1))\n",
    "        # training-----------------------------\n",
    "        train_loss = 0.\n",
    "        for i,(batch_x, batch_y) in enumerate(train_loader):\n",
    "          batch_x = batch_x.cuda()\n",
    "          batch_y = batch_y.cuda()\n",
    "          model.zero_grad()\n",
    "        #           print(batch_x.size())\n",
    "          out= model(batch_x)\n",
    "          _, preds = torch.max(out, 1)\n",
    "          loss = loss_func(out, batch_y.long())\n",
    "          train_loss += loss.item()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          print('epoch:{},batchID{},RLoss{:.6f}'.format(epoch,i,loss.item()))\n",
    "        if epoch%10==9:\n",
    "          state = {'epoch': epoch+1,\n",
    "                   'model_state': model.state_dict(),\n",
    "                   'optimizer_state' : optimizer.state_dict(),}\n",
    "          torch.save(state, './savepath_lstm_Net_6/cls_model'+str(epoch+1)+'.pkl')\n",
    "          record_loss.append(train_loss)\n",
    "        print('Train Loss: {:.6f}'.format(train_loss / (len(train_data))))\n",
    "    print(record_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bda6385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Train Loss: 0.019392\n",
      "epoch 2\n",
      "Train Loss: 0.016296\n",
      "epoch 3\n",
      "Train Loss: 0.015746\n",
      "epoch 4\n",
      "Train Loss: 0.015365\n",
      "epoch 5\n",
      "Train Loss: 0.015231\n",
      "epoch 6\n",
      "Train Loss: 0.014873\n",
      "epoch 7\n",
      "Train Loss: 0.014580\n",
      "epoch 8\n",
      "Train Loss: 0.012185\n",
      "epoch 9\n",
      "Train Loss: 0.011459\n",
      "epoch 10\n",
      "Train Loss: 0.011082\n",
      "epoch 11\n",
      "Train Loss: 0.010806\n",
      "epoch 12\n",
      "Train Loss: 0.010607\n",
      "epoch 13\n",
      "Train Loss: 0.010309\n",
      "epoch 14\n",
      "Train Loss: 0.010076\n",
      "epoch 15\n",
      "Train Loss: 0.009701\n",
      "epoch 16\n",
      "Train Loss: 0.009507\n",
      "epoch 17\n",
      "Train Loss: 0.009455\n",
      "epoch 18\n",
      "Train Loss: 0.009374\n",
      "epoch 19\n",
      "Train Loss: 0.009367\n",
      "epoch 20\n",
      "Train Loss: 0.009299\n",
      "epoch 21\n",
      "Train Loss: 0.009291\n",
      "epoch 22\n",
      "Train Loss: 0.009217\n",
      "epoch 23\n",
      "Train Loss: 0.009220\n",
      "epoch 24\n",
      "Train Loss: 0.009222\n",
      "epoch 25\n",
      "Train Loss: 0.009217\n",
      "epoch 26\n",
      "Train Loss: 0.009206\n",
      "epoch 27\n",
      "Train Loss: 0.009192\n",
      "epoch 28\n",
      "Train Loss: 0.009204\n",
      "epoch 29\n",
      "Train Loss: 0.009187\n",
      "epoch 30\n",
      "Train Loss: 0.009188\n",
      "[297.3212262392044, 249.85754197835922, 241.41895109415054, 235.5831083059311, 233.52069109678268, 228.03062999248505, 223.5380916595459, 186.8220083117485, 175.68779134750366, 169.9071192741394, 165.6801442503929, 162.6330406665802, 158.06286549568176, 154.4799816608429, 148.73152315616608, 145.76561588048935, 144.959807574749, 143.71576911211014, 143.6156587600708, 142.57422769069672, 142.45662599802017, 141.31743341684341, 141.3671973347664, 141.3967616558075, 141.31282967329025, 141.14499986171722, 140.93431943655014, 141.12330323457718, 140.84844559431076, 140.86909121274948]\n"
     ]
    }
   ],
   "source": [
    "# from helpers import get_device, one_hot_embedding\n",
    "from losses import relu_evidence\n",
    "from losses import edl_mse_loss, edl_digamma_loss, edl_log_loss, relu_evidence\n",
    "criterion = edl_digamma_loss\n",
    "if uncertianty==True:\n",
    "    epoch_=30\n",
    "    model = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "    model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-4,weight_decay=0.005)\n",
    "    loss_func = nn.CrossEntropyLoss() \n",
    "    loss_func.cuda() \n",
    "    num=0\n",
    "    model.train()\n",
    "    record_loss=[]\n",
    "    exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=7, gamma=0.1)\n",
    "    for epoch in range(epoch_):\n",
    "        print('epoch {}'.format(epoch + 1))\n",
    "        # training-----------------------------\n",
    "        train_loss = 0.\n",
    "        for i,(batch_x, batch_y) in enumerate(train_loader):\n",
    "          batch_x = batch_x.cuda()\n",
    "          batch_y = batch_y.cuda()\n",
    "          model.zero_grad()\n",
    "        #           print(batch_x.size())\n",
    "          out= model(batch_x)\n",
    "          _, preds = torch.max(out, 1)\n",
    "#           print(out.size(),batch_y.size())\n",
    "          loss = criterion(out, batch_y.float(), epoch, 20, 10)\n",
    "          evidence = relu_evidence(out)\n",
    "          alpha = evidence + 1\n",
    "          u = 20 / torch.sum(alpha, dim=1, keepdim=True)\n",
    "          train_loss += loss.item()\n",
    "\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "#           print('epoch:{},batchID{},RLoss{:.6f}'.format(epoch,i,loss.item()))\n",
    "        exp_lr_scheduler.step()\n",
    "        if epoch%1==0:\n",
    "          state = {'epoch': epoch+1,\n",
    "                   'model_state': model.state_dict(),\n",
    "                   'optimizer_state' : optimizer.state_dict(),}\n",
    "\n",
    "          torch.save(state, './savepath_lstm_Net_6/cls_evi_model'+str(epoch+1)+'.pkl')\n",
    "          record_loss.append(train_loss)\n",
    "        print('Train Loss: {:.6f}'.format(train_loss / (len(train_data))))\n",
    "    print(record_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc_001",
   "language": "python",
   "name": "sc_001"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
